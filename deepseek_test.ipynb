{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOr8hc4BA4x/mSFKwUN6OWN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xjdeng/llm-colab-notebooks/blob/main/deepseek_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "cellView": "form",
        "id": "H67iGladFNKH",
        "outputId": "ef0b52ef-e5c6-40d2-8b15-4d603df4fd98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'text-generation-webui'...\n",
            "remote: Enumerating objects: 20138, done.\u001b[K\n",
            "remote: Counting objects: 100% (171/171), done.\u001b[K\n",
            "remote: Compressing objects: 100% (73/73), done.\u001b[K\n",
            "remote: Total 20138 (delta 143), reused 101 (delta 98), pack-reused 19967 (from 3)\u001b[K\n",
            "Receiving objects: 100% (20138/20138), 29.15 MiB | 12.38 MiB/s, done.\n",
            "Resolving deltas: 100% (14337/14337), done.\n",
            "/content/text-generation-webui\n",
            "Collecting llama-cpp-python==0.3.7+cpuavx2 (from -r requirements.txt (line 35))\n",
            "  Downloading https://github.com/oobabooga/llama-cpp-python-cuBLAS-wheels/releases/download/cpu/llama_cpp_python-0.3.7+cpuavx2-cp311-cp311-linux_x86_64.whl (4.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hIgnoring llama-cpp-python: markers 'platform_system == \"Linux\" and platform_machine == \"x86_64\" and python_version == \"3.10\"' don't match your environment\n",
            "Ignoring llama-cpp-python: markers 'platform_system == \"Windows\" and python_version == \"3.11\"' don't match your environment\n",
            "Ignoring llama-cpp-python: markers 'platform_system == \"Windows\" and python_version == \"3.10\"' don't match your environment\n",
            "Ignoring llama-cpp-python-cuda: markers 'platform_system == \"Windows\" and python_version == \"3.11\"' don't match your environment\n",
            "Ignoring llama-cpp-python-cuda: markers 'platform_system == \"Windows\" and python_version == \"3.10\"' don't match your environment\n",
            "Collecting llama-cpp-python-cuda==0.3.7+cu121 (from -r requirements.txt (line 43))\n",
            "  Downloading https://github.com/oobabooga/llama-cpp-python-cuBLAS-wheels/releases/download/textgen-webui/llama_cpp_python_cuda-0.3.7+cu121-cp311-cp311-linux_x86_64.whl (452.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.3/452.3 MB\u001b[0m \u001b[31m955.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hIgnoring llama-cpp-python-cuda: markers 'platform_system == \"Linux\" and platform_machine == \"x86_64\" and python_version == \"3.10\"' don't match your environment\n",
            "Ignoring llama-cpp-python-cuda-tensorcores: markers 'platform_system == \"Windows\" and python_version == \"3.11\"' don't match your environment\n",
            "Ignoring llama-cpp-python-cuda-tensorcores: markers 'platform_system == \"Windows\" and python_version == \"3.10\"' don't match your environment\n",
            "Collecting llama-cpp-python-cuda-tensorcores==0.3.7+cu121 (from -r requirements.txt (line 49))\n",
            "  Downloading https://github.com/oobabooga/llama-cpp-python-cuBLAS-wheels/releases/download/textgen-webui/llama_cpp_python_cuda_tensorcores-0.3.7+cu121-cp311-cp311-linux_x86_64.whl (488.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m488.9/488.9 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hIgnoring llama-cpp-python-cuda-tensorcores: markers 'platform_system == \"Linux\" and platform_machine == \"x86_64\" and python_version == \"3.10\"' don't match your environment\n",
            "Ignoring exllamav2: markers 'platform_system == \"Windows\" and python_version == \"3.11\"' don't match your environment\n",
            "Ignoring exllamav2: markers 'platform_system == \"Windows\" and python_version == \"3.10\"' don't match your environment\n",
            "Collecting exllamav2==0.2.7+cu121.torch2.4.1 (from -r requirements.txt (line 55))\n",
            "  Downloading https://github.com/oobabooga/exllamav2/releases/download/v0.2.7/exllamav2-0.2.7+cu121.torch2.4.1-cp311-cp311-linux_x86_64.whl (137.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.1/137.1 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hIgnoring exllamav2: markers 'platform_system == \"Linux\" and platform_machine == \"x86_64\" and python_version == \"3.10\"' don't match your environment\n",
            "Ignoring exllamav2: markers 'platform_system == \"Linux\" and platform_machine != \"x86_64\"' don't match your environment\n",
            "Ignoring flash-attn: markers 'platform_system == \"Windows\" and python_version == \"3.11\"' don't match your environment\n",
            "Ignoring flash-attn: markers 'platform_system == \"Windows\" and python_version == \"3.10\"' don't match your environment\n",
            "Collecting flash-attn==2.7.3+cu12torch2.4cxx11abiFALSE (from -r requirements.txt (line 60))\n",
            "  Downloading https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.3/flash_attn-2.7.3+cu12torch2.4cxx11abiFALSE-cp311-cp311-linux_x86_64.whl (191.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m191.4/191.4 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hIgnoring flash-attn: markers 'platform_system == \"Linux\" and platform_machine == \"x86_64\" and python_version == \"3.10\"' don't match your environment\n",
            "Requirement already satisfied: accelerate==1.3.* in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (1.3.0)\n",
            "Collecting bitsandbytes==0.45.* (from -r requirements.txt (line 2))\n",
            "  Downloading bitsandbytes-0.45.2-py3-none-manylinux_2_24_x86_64.whl.metadata (5.8 kB)\n",
            "Collecting colorama (from -r requirements.txt (line 3))\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Collecting datasets (from -r requirements.txt (line 4))\n",
            "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 5)) (0.8.0)\n",
            "Collecting fastapi==0.112.4 (from -r requirements.txt (line 6))\n",
            "  Downloading fastapi-0.112.4-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting gradio==4.37.* (from -r requirements.txt (line 7))\n",
            "  Downloading gradio-4.37.2-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: jinja2==3.1.5 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 8)) (3.1.5)\n",
            "Requirement already satisfied: markdown in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 9)) (3.7)\n",
            "Collecting numba==0.59.* (from -r requirements.txt (line 10))\n",
            "  Downloading numba-0.59.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: numpy==1.26.* in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 11)) (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 12)) (2.2.2)\n",
            "Collecting peft==0.12.* (from -r requirements.txt (line 13))\n",
            "  Downloading peft-0.12.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: Pillow>=9.5.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 14)) (11.1.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 15)) (5.9.5)\n",
            "Collecting pydantic==2.8.2 (from -r requirements.txt (line 16))\n",
            "  Downloading pydantic-2.8.2-py3-none-any.whl.metadata (125 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.2/125.2 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 17)) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 18)) (2.32.3)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 19)) (13.9.4)\n",
            "Requirement already satisfied: safetensors==0.5.* in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 20)) (0.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 21)) (1.13.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 22)) (0.2.0)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 23)) (2.18.0)\n",
            "Requirement already satisfied: transformers==4.48.* in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 24)) (4.48.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 25)) (4.67.1)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 26)) (0.19.5)\n",
            "Collecting SpeechRecognition==3.10.0 (from -r requirements.txt (line 29))\n",
            "  Downloading SpeechRecognition-3.10.0-py2.py3-none-any.whl.metadata (28 kB)\n",
            "Collecting flask_cloudflared==0.0.14 (from -r requirements.txt (line 30))\n",
            "  Downloading flask_cloudflared-0.0.14-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting sse-starlette==1.6.5 (from -r requirements.txt (line 31))\n",
            "  Downloading sse_starlette-1.6.5-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting tiktoken (from -r requirements.txt (line 32))\n",
            "  Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate==1.3.*->-r requirements.txt (line 1)) (24.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate==1.3.*->-r requirements.txt (line 1)) (2.5.1+cu124)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate==1.3.*->-r requirements.txt (line 1)) (0.28.1)\n",
            "Collecting starlette<0.39.0,>=0.37.2 (from fastapi==0.112.4->-r requirements.txt (line 6))\n",
            "  Downloading starlette-0.38.6-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi==0.112.4->-r requirements.txt (line 6)) (4.12.2)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio==4.37.*->-r requirements.txt (line 7))\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.37.*->-r requirements.txt (line 7)) (5.5.0)\n",
            "Collecting ffmpy (from gradio==4.37.*->-r requirements.txt (line 7))\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.0.2 (from gradio==4.37.*->-r requirements.txt (line 7))\n",
            "  Downloading gradio_client-1.0.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio==4.37.*->-r requirements.txt (line 7)) (0.28.1)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.11/dist-packages (from gradio==4.37.*->-r requirements.txt (line 7)) (6.5.2)\n",
            "Collecting markupsafe~=2.0 (from gradio==4.37.*->-r requirements.txt (line 7))\n",
            "  Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.37.*->-r requirements.txt (line 7)) (3.10.0)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.37.*->-r requirements.txt (line 7)) (3.10.15)\n",
            "Collecting Pillow>=9.5.0 (from -r requirements.txt (line 14))\n",
            "  Downloading pillow-10.4.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
            "Collecting pydub (from gradio==4.37.*->-r requirements.txt (line 7))\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.9 (from gradio==4.37.*->-r requirements.txt (line 7))\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting ruff>=0.2.2 (from gradio==4.37.*->-r requirements.txt (line 7))\n",
            "  Downloading ruff-0.9.5-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio==4.37.*->-r requirements.txt (line 7))\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio==4.37.*->-r requirements.txt (line 7))\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio==4.37.*->-r requirements.txt (line 7)) (0.15.1)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.37.*->-r requirements.txt (line 7)) (2.3.0)\n",
            "Collecting uvicorn>=0.14.0 (from gradio==4.37.*->-r requirements.txt (line 7))\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting llvmlite<0.43,>=0.42.0dev0 (from numba==0.59.*->-r requirements.txt (line 10))\n",
            "  Downloading llvmlite-0.42.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic==2.8.2->-r requirements.txt (line 16)) (0.7.0)\n",
            "Collecting pydantic-core==2.20.1 (from pydantic==2.8.2->-r requirements.txt (line 16))\n",
            "  Downloading pydantic_core-2.20.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.48.*->-r requirements.txt (line 24)) (3.17.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.48.*->-r requirements.txt (line 24)) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers==4.48.*->-r requirements.txt (line 24)) (0.21.0)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.11/dist-packages (from flask_cloudflared==0.0.14->-r requirements.txt (line 30)) (3.1.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.0.2->gradio==4.37.*->-r requirements.txt (line 7)) (2024.10.0)\n",
            "Collecting websockets<12.0,>=10.0 (from gradio-client==1.0.2->gradio==4.37.*->-r requirements.txt (line 7))\n",
            "  Downloading websockets-11.0.3-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets->-r requirements.txt (line 4)) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets->-r requirements.txt (line 4))\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting xxhash (from datasets->-r requirements.txt (line 4))\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets->-r requirements.txt (line 4))\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec (from gradio-client==1.0.2->gradio==4.37.*->-r requirements.txt (line 7))\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets->-r requirements.txt (line 4)) (3.11.11)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->-r requirements.txt (line 12)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->-r requirements.txt (line 12)) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->-r requirements.txt (line 12)) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->-r requirements.txt (line 18)) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->-r requirements.txt (line 18)) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->-r requirements.txt (line 18)) (2025.1.31)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->-r requirements.txt (line 19)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->-r requirements.txt (line 19)) (2.18.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 23)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 23)) (1.70.0)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 23)) (4.25.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 23)) (75.1.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 23)) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 23)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 23)) (3.1.3)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 26)) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 26)) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 26)) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 26)) (4.3.6)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 26)) (2.20.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 26)) (1.3.4)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python==0.3.7+cpuavx2->-r requirements.txt (line 35))\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting ninja (from exllamav2==0.2.7+cu121.torch2.4.1->-r requirements.txt (line 55))\n",
            "  Downloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.3 kB)\n",
            "Collecting fastparquet (from exllamav2==0.2.7+cu121.torch2.4.1->-r requirements.txt (line 55))\n",
            "  Downloading fastparquet-2024.11.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6.0,>=4.2.0->gradio==4.37.*->-r requirements.txt (line 7)) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6.0,>=4.2.0->gradio==4.37.*->-r requirements.txt (line 7)) (1.25.0)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from Flask>=0.8->flask_cloudflared==0.0.14->-r requirements.txt (line 30)) (2.2.0)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from Flask>=0.8->flask_cloudflared==0.0.14->-r requirements.txt (line 30)) (1.9.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->-r requirements.txt (line 4)) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->-r requirements.txt (line 4)) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->-r requirements.txt (line 4)) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->-r requirements.txt (line 4)) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->-r requirements.txt (line 4)) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->-r requirements.txt (line 4)) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->-r requirements.txt (line 4)) (1.18.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 26)) (4.0.12)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio==4.37.*->-r requirements.txt (line 7)) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio==4.37.*->-r requirements.txt (line 7)) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio==4.37.*->-r requirements.txt (line 7)) (0.14.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->-r requirements.txt (line 19)) (0.1.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.0->gradio==4.37.*->-r requirements.txt (line 7)) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.0->gradio==4.37.*->-r requirements.txt (line 7)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.0->gradio==4.37.*->-r requirements.txt (line 7)) (4.55.8)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.0->gradio==4.37.*->-r requirements.txt (line 7)) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.0->gradio==4.37.*->-r requirements.txt (line 7)) (3.2.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.3.*->-r requirements.txt (line 1)) (3.4.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->accelerate==1.3.*->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->accelerate==1.3.*->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->accelerate==1.3.*->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->accelerate==1.3.*->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->accelerate==1.3.*->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->accelerate==1.3.*->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->accelerate==1.3.*->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->accelerate==1.3.*->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->accelerate==1.3.*->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.3.*->-r requirements.txt (line 1)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.3.*->-r requirements.txt (line 1)) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->accelerate==1.3.*->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.3.*->-r requirements.txt (line 1)) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.3.*->-r requirements.txt (line 1)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate==1.3.*->-r requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio==4.37.*->-r requirements.txt (line 7)) (1.5.4)\n",
            "Requirement already satisfied: cramjam>=2.3 in /usr/local/lib/python3.11/dist-packages (from fastparquet->exllamav2==0.2.7+cu121.torch2.4.1->-r requirements.txt (line 55)) (2.9.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.24.1->gradio==4.37.*->-r requirements.txt (line 7)) (1.3.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 26)) (5.0.2)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.37.*->-r requirements.txt (line 7)) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.37.*->-r requirements.txt (line 7)) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.37.*->-r requirements.txt (line 7)) (0.22.3)\n",
            "Downloading bitsandbytes-0.45.2-py3-none-manylinux_2_24_x86_64.whl (69.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.112.4-py3-none-any.whl (93 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.9/93.9 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio-4.37.2-py3-none-any.whl (12.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m100.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numba-0.59.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m99.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading peft-0.12.0-py3-none-any.whl (296 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic-2.8.2-py3-none-any.whl (423 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m423.9/423.9 kB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading SpeechRecognition-3.10.0-py2.py3-none-any.whl (32.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.8/32.8 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading flask_cloudflared-0.0.14-py3-none-any.whl (6.4 kB)\n",
            "Downloading sse_starlette-1.6.5-py3-none-any.whl (9.6 kB)\n",
            "Downloading gradio_client-1.0.2-py3-none-any.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.2/318.2 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_core-2.20.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m99.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-10.4.0-cp311-cp311-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m107.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llvmlite-0.42.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n",
            "Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.9.5-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m106.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.38.6-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m96.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m66.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m78.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-11.0.3-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.6/130.6 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastparquet-2024.11.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m91.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.9/422.9 kB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydub, xxhash, websockets, uvicorn, tomlkit, semantic-version, ruff, python-multipart, pydantic-core, Pillow, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, ninja, markupsafe, llvmlite, fsspec, ffmpy, diskcache, dill, colorama, aiofiles, tiktoken, starlette, SpeechRecognition, pydantic, nvidia-cusparse-cu12, nvidia-cudnn-cu12, numba, multiprocess, sse-starlette, nvidia-cusolver-cu12, llama-cpp-python-cuda-tensorcores, llama-cpp-python-cuda, llama-cpp-python, gradio-client, fastparquet, fastapi, flask_cloudflared, exllamav2, datasets, gradio, flash-attn, bitsandbytes, peft\n",
            "  Attempting uninstall: websockets\n",
            "    Found existing installation: websockets 14.2\n",
            "    Uninstalling websockets-14.2:\n",
            "      Successfully uninstalled websockets-14.2\n",
            "  Attempting uninstall: pydantic-core\n",
            "    Found existing installation: pydantic_core 2.27.2\n",
            "    Uninstalling pydantic_core-2.27.2:\n",
            "      Successfully uninstalled pydantic_core-2.27.2\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: pillow 11.1.0\n",
            "    Uninstalling pillow-11.1.0:\n",
            "      Successfully uninstalled pillow-11.1.0\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "  Attempting uninstall: llvmlite\n",
            "    Found existing installation: llvmlite 0.44.0\n",
            "    Uninstalling llvmlite-0.44.0:\n",
            "      Successfully uninstalled llvmlite-0.44.0\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.10.6\n",
            "    Uninstalling pydantic-2.10.6:\n",
            "      Successfully uninstalled pydantic-2.10.6\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: numba\n",
            "    Found existing installation: numba 0.61.0\n",
            "    Uninstalling numba-0.61.0:\n",
            "      Successfully uninstalled numba-0.61.0\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: peft\n",
            "    Found existing installation: peft 0.14.0\n",
            "    Uninstalling peft-0.14.0:\n",
            "      Successfully uninstalled peft-0.14.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albumentations 2.0.3 requires pydantic>=2.9.2, but you have pydantic 2.8.2 which is incompatible.\n",
            "google-genai 0.8.0 requires websockets<15.0dev,>=13.0, but you have websockets 11.0.3 which is incompatible.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Pillow-10.4.0 SpeechRecognition-3.10.0 aiofiles-23.2.1 bitsandbytes-0.45.2 colorama-0.4.6 datasets-3.2.0 dill-0.3.8 diskcache-5.6.3 exllamav2-0.2.7+cu121.torch2.4.1 fastapi-0.112.4 fastparquet-2024.11.0 ffmpy-0.5.0 flash-attn-2.7.3 flask_cloudflared-0.0.14 fsspec-2024.9.0 gradio-4.37.2 gradio-client-1.0.2 llama-cpp-python-0.3.7+cpuavx2 llama-cpp-python-cuda-0.3.7+cu121 llama-cpp-python-cuda-tensorcores-0.3.7+cu121 llvmlite-0.42.0 markupsafe-2.1.5 multiprocess-0.70.16 ninja-1.11.1.3 numba-0.59.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 peft-0.12.0 pydantic-2.8.2 pydantic-core-2.20.1 pydub-0.25.1 python-multipart-0.0.20 ruff-0.9.5 semantic-version-2.10.0 sse-starlette-1.6.5 starlette-0.38.6 tiktoken-0.8.0 tomlkit-0.12.0 uvicorn-0.34.0 websockets-11.0.3 xxhash-3.5.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              },
              "id": "1a691c60de4c436f8dc4b4a85c4b1bcb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting num2words\n",
            "  Downloading num2words-0.5.14-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting omegaconf\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting docopt>=0.6.2 (from num2words)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting antlr4-python3-runtime==4.9.* (from omegaconf)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from omegaconf) (6.0.2)\n",
            "Downloading num2words-0.5.14-py3-none-any.whl (163 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.5/163.5 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: antlr4-python3-runtime, docopt\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144555 sha256=d0771c50844ebfddb98eab04f66b430af02196f39e00c165d19df329a1624dae\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/97/32/461f837398029ad76911109f07047fde1d7b661a147c7c56d1\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=38b9cf1bece3708d7bf646c189fab618dfa6bfb92dc78ccb5049d46ef225669f\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/b0/8c/4b75c4116c31f83c8f9f047231251e13cc74481cca4a78a9ce\n",
            "Successfully built antlr4-python3-runtime docopt\n",
            "Installing collected packages: docopt, antlr4-python3-runtime, omegaconf, num2words\n",
            "Successfully installed antlr4-python3-runtime-4.9.3 docopt-0.6.2 num2words-0.5.14 omegaconf-2.3.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              },
              "id": "9eac9c7ce82443b0a6937ae1290d9acf"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "#@title Initial Setup\n",
        "\n",
        "\n",
        "%cd /content\n",
        "!git clone https://github.com/oobabooga/text-generation-webui\n",
        "%cd text-generation-webui\n",
        "\n",
        "!pip install -r requirements.txt\n",
        "!pip install num2words omegaconf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download the Model\n",
        "model = \"mradermacher/DeepSeek-R1-Distill-Qwen-14B-Uncensored-GGUF\" #@param {type:\"string\"}\n",
        "!python download-model.py $model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "zq4a8RtEFhC5",
        "outputId": "4dbba4da-aa96-48e0-b61b-0e6d756ccc44"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading the model to models/mradermacher_DeepSeek-R1-Distill-Qwen-14B-Uncensored-GGUF\n",
            "DeepSeek-R1-Distill-Qwen-14B-Uncensored.Q4_K_M.gguf: 100% 8.37G/8.37G [03:34<00:00, 42.0MB/s]\n",
            "\n",
            "Download of 1 files to models/mradermacher_DeepSeek-R1-Distill-Qwen-14B-Uncensored-GGUF completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Click the 2nd link that appears\n",
        "!python server.py --share"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "collapsed": true,
        "id": "_947JWNVFhXt",
        "outputId": "3eb500c4-2ac3-47e6-aac1-54ec0af0c0c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2;36m21:16:48-131317\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Starting Text generation web UI                                            \n",
            "\n",
            "Running on local URL:  http://127.0.0.1:7860\n",
            "\n",
            "Running on public URL: https://89583a441dea8cae3e.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n",
            "\u001b[2;36m21:19:11-860486\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading \u001b[32m\"mradermacher_DeepSeek-R1-Distill-Qwen-14B-Uncensored-GGUF\"\u001b[0m        \n",
            "\u001b[2;36m21:19:12-612643\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m llama.cpp weights detected:                                                \n",
            "\u001b[2;36m                \u001b[0m         \u001b[32m\"models/mradermacher_DeepSeek-R1-Distill-Qwen-14B-Uncensored-GGUF/DeepSeek-\u001b[0m\n",
            "\u001b[2;36m                \u001b[0m         \u001b[32mR1-Distill-Qwen-14B-Uncensored.Q4_K_M.gguf\"\u001b[0m                                \n",
            "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    yes\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
            "ggml_cuda_init: found 1 CUDA devices:\n",
            "  Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
            "llama_model_load_from_file_impl: using device CUDA0 (Tesla T4) - 14992 MiB free\n",
            "llama_model_loader: loaded meta data with 44 key-value pairs and 579 tensors from models/mradermacher_DeepSeek-R1-Distill-Qwen-14B-Uncensored-GGUF/DeepSeek-R1-Distill-Qwen-14B-Uncensored.Q4_K_M.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 14B Uncensored\n",
            "llama_model_loader: - kv   3:                           general.finetune str              = Uncensored\n",
            "llama_model_loader: - kv   4:                           general.basename str              = DeepSeek-R1-Distill-Qwen\n",
            "llama_model_loader: - kv   5:                         general.size_label str              = 14B\n",
            "llama_model_loader: - kv   6:                            general.license str              = mit\n",
            "llama_model_loader: - kv   7:                   general.base_model.count u32              = 1\n",
            "llama_model_loader: - kv   8:                  general.base_model.0.name str              = DeepSeek R1 Distill Qwen 14B\n",
            "llama_model_loader: - kv   9:          general.base_model.0.organization str              = Deepseek Ai\n",
            "llama_model_loader: - kv  10:              general.base_model.0.repo_url str              = https://huggingface.co/deepseek-ai/De...\n",
            "llama_model_loader: - kv  11:                      general.dataset.count u32              = 1\n",
            "llama_model_loader: - kv  12:                     general.dataset.0.name str              = Uncensor\n",
            "llama_model_loader: - kv  13:             general.dataset.0.organization str              = Guilherme34\n",
            "llama_model_loader: - kv  14:                 general.dataset.0.repo_url str              = https://huggingface.co/Guilherme34/un...\n",
            "llama_model_loader: - kv  15:                               general.tags arr[str,1]       = [\"generated_from_trainer\"]\n",
            "llama_model_loader: - kv  16:                          qwen2.block_count u32              = 48\n",
            "llama_model_loader: - kv  17:                       qwen2.context_length u32              = 131072\n",
            "llama_model_loader: - kv  18:                     qwen2.embedding_length u32              = 5120\n",
            "llama_model_loader: - kv  19:                  qwen2.feed_forward_length u32              = 13824\n",
            "llama_model_loader: - kv  20:                 qwen2.attention.head_count u32              = 40\n",
            "llama_model_loader: - kv  21:              qwen2.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  22:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv  23:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  24:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  25:                         tokenizer.ggml.pre str              = deepseek-r1-qwen\n",
            "llama_model_loader: - kv  26:                      tokenizer.ggml.tokens arr[str,151665]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  27:                  tokenizer.ggml.token_type arr[i32,151665]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  28:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
            "llama_model_loader: - kv  29:                tokenizer.ggml.bos_token_id u32              = 151646\n",
            "llama_model_loader: - kv  30:                tokenizer.ggml.eos_token_id u32              = 151643\n",
            "llama_model_loader: - kv  31:            tokenizer.ggml.padding_token_id u32              = 151643\n",
            "llama_model_loader: - kv  32:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  33:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  34:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\n",
            "llama_model_loader: - kv  35:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  36:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  37:                                general.url str              = https://huggingface.co/mradermacher/D...\n",
            "llama_model_loader: - kv  38:              mradermacher.quantize_version str              = 2\n",
            "llama_model_loader: - kv  39:                  mradermacher.quantized_by str              = mradermacher\n",
            "llama_model_loader: - kv  40:                  mradermacher.quantized_at str              = 2025-01-25T02:56:59+01:00\n",
            "llama_model_loader: - kv  41:                  mradermacher.quantized_on str              = marco\n",
            "llama_model_loader: - kv  42:                         general.source.url str              = https://huggingface.co/nicoboss/DeepS...\n",
            "llama_model_loader: - kv  43:                  mradermacher.convert_type str              = hf\n",
            "llama_model_loader: - type  f32:  241 tensors\n",
            "llama_model_loader: - type q4_K:  289 tensors\n",
            "llama_model_loader: - type q6_K:   49 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = Q4_K - Medium\n",
            "print_info: file size   = 8.36 GiB (4.86 BPW) \n",
            "init_tokenizer: initializing tokenizer for type 2\n",
            "load: control token: 151660 '<|fim_middle|>' is not marked as EOG\n",
            "load: control token: 151659 '<|fim_prefix|>' is not marked as EOG\n",
            "load: control token: 151653 '<|vision_end|>' is not marked as EOG\n",
            "load: control token: 151645 '<｜Assistant｜>' is not marked as EOG\n",
            "load: control token: 151644 '<｜User｜>' is not marked as EOG\n",
            "load: control token: 151655 '<|image_pad|>' is not marked as EOG\n",
            "load: control token: 151651 '<|quad_end|>' is not marked as EOG\n",
            "load: control token: 151646 '<｜begin▁of▁sentence｜>' is not marked as EOG\n",
            "load: control token: 151643 '<｜end▁of▁sentence｜>' is not marked as EOG\n",
            "load: control token: 151652 '<|vision_start|>' is not marked as EOG\n",
            "load: control token: 151647 '<|EOT|>' is not marked as EOG\n",
            "load: control token: 151654 '<|vision_pad|>' is not marked as EOG\n",
            "load: control token: 151656 '<|video_pad|>' is not marked as EOG\n",
            "load: control token: 151661 '<|fim_suffix|>' is not marked as EOG\n",
            "load: control token: 151650 '<|quad_start|>' is not marked as EOG\n",
            "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
            "load: special tokens cache size = 22\n",
            "load: token to piece cache size = 0.9310 MB\n",
            "print_info: arch             = qwen2\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 131072\n",
            "print_info: n_embd           = 5120\n",
            "print_info: n_layer          = 48\n",
            "print_info: n_head           = 40\n",
            "print_info: n_head_kv        = 8\n",
            "print_info: n_rot            = 128\n",
            "print_info: n_swa            = 0\n",
            "print_info: n_embd_head_k    = 128\n",
            "print_info: n_embd_head_v    = 128\n",
            "print_info: n_gqa            = 5\n",
            "print_info: n_embd_k_gqa     = 1024\n",
            "print_info: n_embd_v_gqa     = 1024\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-05\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: n_ff             = 13824\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 2\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 1000000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 131072\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: ssm_d_conv       = 0\n",
            "print_info: ssm_d_inner      = 0\n",
            "print_info: ssm_d_state      = 0\n",
            "print_info: ssm_dt_rank      = 0\n",
            "print_info: ssm_dt_b_c_rms   = 0\n",
            "print_info: model type       = 14B\n",
            "print_info: model params     = 14.77 B\n",
            "print_info: general.name     = DeepSeek R1 Distill Qwen 14B Uncensored\n",
            "print_info: vocab type       = BPE\n",
            "print_info: n_vocab          = 151665\n",
            "print_info: n_merges         = 151387\n",
            "print_info: BOS token        = 151646 '<｜begin▁of▁sentence｜>'\n",
            "print_info: EOS token        = 151643 '<｜end▁of▁sentence｜>'\n",
            "print_info: EOT token        = 151643 '<｜end▁of▁sentence｜>'\n",
            "print_info: PAD token        = 151643 '<｜end▁of▁sentence｜>'\n",
            "print_info: LF token         = 148848 'ÄĬ'\n",
            "print_info: FIM PRE token    = 151659 '<|fim_prefix|>'\n",
            "print_info: FIM SUF token    = 151661 '<|fim_suffix|>'\n",
            "print_info: FIM MID token    = 151660 '<|fim_middle|>'\n",
            "print_info: FIM PAD token    = 151662 '<|fim_pad|>'\n",
            "print_info: FIM REP token    = 151663 '<|repo_name|>'\n",
            "print_info: FIM SEP token    = 151664 '<|file_sep|>'\n",
            "print_info: EOG token        = 151643 '<｜end▁of▁sentence｜>'\n",
            "print_info: EOG token        = 151662 '<|fim_pad|>'\n",
            "print_info: EOG token        = 151663 '<|repo_name|>'\n",
            "print_info: EOG token        = 151664 '<|file_sep|>'\n",
            "print_info: max token length = 256\n",
            "load_tensors: layer   0 assigned to device CUDA0\n",
            "load_tensors: layer   1 assigned to device CUDA0\n",
            "load_tensors: layer   2 assigned to device CUDA0\n",
            "load_tensors: layer   3 assigned to device CUDA0\n",
            "load_tensors: layer   4 assigned to device CUDA0\n",
            "load_tensors: layer   5 assigned to device CUDA0\n",
            "load_tensors: layer   6 assigned to device CUDA0\n",
            "load_tensors: layer   7 assigned to device CUDA0\n",
            "load_tensors: layer   8 assigned to device CUDA0\n",
            "load_tensors: layer   9 assigned to device CUDA0\n",
            "load_tensors: layer  10 assigned to device CUDA0\n",
            "load_tensors: layer  11 assigned to device CUDA0\n",
            "load_tensors: layer  12 assigned to device CUDA0\n",
            "load_tensors: layer  13 assigned to device CUDA0\n",
            "load_tensors: layer  14 assigned to device CUDA0\n",
            "load_tensors: layer  15 assigned to device CUDA0\n",
            "load_tensors: layer  16 assigned to device CUDA0\n",
            "load_tensors: layer  17 assigned to device CUDA0\n",
            "load_tensors: layer  18 assigned to device CUDA0\n",
            "load_tensors: layer  19 assigned to device CUDA0\n",
            "load_tensors: layer  20 assigned to device CUDA0\n",
            "load_tensors: layer  21 assigned to device CUDA0\n",
            "load_tensors: layer  22 assigned to device CUDA0\n",
            "load_tensors: layer  23 assigned to device CUDA0\n",
            "load_tensors: layer  24 assigned to device CUDA0\n",
            "load_tensors: layer  25 assigned to device CUDA0\n",
            "load_tensors: layer  26 assigned to device CUDA0\n",
            "load_tensors: layer  27 assigned to device CUDA0\n",
            "load_tensors: layer  28 assigned to device CUDA0\n",
            "load_tensors: layer  29 assigned to device CUDA0\n",
            "load_tensors: layer  30 assigned to device CUDA0\n",
            "load_tensors: layer  31 assigned to device CUDA0\n",
            "load_tensors: layer  32 assigned to device CUDA0\n",
            "load_tensors: layer  33 assigned to device CUDA0\n",
            "load_tensors: layer  34 assigned to device CUDA0\n",
            "load_tensors: layer  35 assigned to device CUDA0\n",
            "load_tensors: layer  36 assigned to device CUDA0\n",
            "load_tensors: layer  37 assigned to device CUDA0\n",
            "load_tensors: layer  38 assigned to device CUDA0\n",
            "load_tensors: layer  39 assigned to device CUDA0\n",
            "load_tensors: layer  40 assigned to device CUDA0\n",
            "load_tensors: layer  41 assigned to device CUDA0\n",
            "load_tensors: layer  42 assigned to device CUDA0\n",
            "load_tensors: layer  43 assigned to device CUDA0\n",
            "load_tensors: layer  44 assigned to device CUDA0\n",
            "load_tensors: layer  45 assigned to device CUDA0\n",
            "load_tensors: layer  46 assigned to device CUDA0\n",
            "load_tensors: layer  47 assigned to device CUDA0\n",
            "load_tensors: layer  48 assigned to device CUDA0\n",
            "load_tensors: tensor 'token_embd.weight' (q4_K) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
            "load_tensors: offloading 48 repeating layers to GPU\n",
            "load_tensors: offloading output layer to GPU\n",
            "load_tensors: offloaded 49/49 layers to GPU\n",
            "load_tensors:        CUDA0 model buffer size =  8146.78 MiB\n",
            "load_tensors:   CPU_Mapped model buffer size =   416.56 MiB\n",
            "llama_init_from_model: V cache quantization requires flash_attn\n",
            "\u001b[2;36m21:19:52-098212\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;31mERROR   \u001b[0m Failed to load the model.                                                  \n",
            "Traceback (most recent call last):\n",
            "  File \"/content/text-generation-webui/modules/llamacpp_model.py\", line 112, in from_pretrained\n",
            "    result.model = Llama(**params)\n",
            "                   ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/llama_cpp_cuda/llama.py\", line 393, in __init__\n",
            "    internals.LlamaContext(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/llama_cpp_cuda/_internals.py\", line 255, in __init__\n",
            "    raise ValueError(\"Failed to create llama_context\")\n",
            "ValueError: Failed to create llama_context\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/text-generation-webui/modules/ui_model_menu.py\", line 214, in load_model_wrapper\n",
            "    shared.model, shared.tokenizer = load_model(selected_model, loader)\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/text-generation-webui/modules/models.py\", line 90, in load_model\n",
            "    output = load_func_map[loader](model_name)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/text-generation-webui/modules/models.py\", line 280, in llamacpp_loader\n",
            "    model, tokenizer = LlamaCppModel.from_pretrained(model_file)\n",
            "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/text-generation-webui/modules/llamacpp_model.py\", line 122, in from_pretrained\n",
            "    raise type(e)(error_message) from e\n",
            "ValueError: Failed loading the model. **This usually happens due to lack of memory**. Try these steps:\n",
            "1. Reduce the context length `n_ctx` (currently 131072). Try a lower value like 4096.\n",
            "2. Lower the `n-gpu-layers` value (currently 49).\n",
            "\n",
            "Exception ignored in: <function LlamaCppModel.__del__ at 0x7871d07f8a40>\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/text-generation-webui/modules/llamacpp_model.py\", line 62, in __del__\n",
            "    del self.model\n",
            "        ^^^^^^^^^^\n",
            "AttributeError: 'LlamaCppModel' object has no attribute 'model'\n",
            "\u001b[2;36m21:20:02-040483\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading \u001b[32m\"mradermacher_DeepSeek-R1-Distill-Qwen-14B-Uncensored-GGUF\"\u001b[0m        \n",
            "\u001b[2;36m21:20:02-530477\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m llama.cpp weights detected:                                                \n",
            "\u001b[2;36m                \u001b[0m         \u001b[32m\"models/mradermacher_DeepSeek-R1-Distill-Qwen-14B-Uncensored-GGUF/DeepSeek-\u001b[0m\n",
            "\u001b[2;36m                \u001b[0m         \u001b[32mR1-Distill-Qwen-14B-Uncensored.Q4_K_M.gguf\"\u001b[0m                                \n",
            "llama_model_load_from_file_impl: using device CUDA0 (Tesla T4) - 14992 MiB free\n",
            "llama_model_loader: loaded meta data with 44 key-value pairs and 579 tensors from models/mradermacher_DeepSeek-R1-Distill-Qwen-14B-Uncensored-GGUF/DeepSeek-R1-Distill-Qwen-14B-Uncensored.Q4_K_M.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 14B Uncensored\n",
            "llama_model_loader: - kv   3:                           general.finetune str              = Uncensored\n",
            "llama_model_loader: - kv   4:                           general.basename str              = DeepSeek-R1-Distill-Qwen\n",
            "llama_model_loader: - kv   5:                         general.size_label str              = 14B\n",
            "llama_model_loader: - kv   6:                            general.license str              = mit\n",
            "llama_model_loader: - kv   7:                   general.base_model.count u32              = 1\n",
            "llama_model_loader: - kv   8:                  general.base_model.0.name str              = DeepSeek R1 Distill Qwen 14B\n",
            "llama_model_loader: - kv   9:          general.base_model.0.organization str              = Deepseek Ai\n",
            "llama_model_loader: - kv  10:              general.base_model.0.repo_url str              = https://huggingface.co/deepseek-ai/De...\n",
            "llama_model_loader: - kv  11:                      general.dataset.count u32              = 1\n",
            "llama_model_loader: - kv  12:                     general.dataset.0.name str              = Uncensor\n",
            "llama_model_loader: - kv  13:             general.dataset.0.organization str              = Guilherme34\n",
            "llama_model_loader: - kv  14:                 general.dataset.0.repo_url str              = https://huggingface.co/Guilherme34/un...\n",
            "llama_model_loader: - kv  15:                               general.tags arr[str,1]       = [\"generated_from_trainer\"]\n",
            "llama_model_loader: - kv  16:                          qwen2.block_count u32              = 48\n",
            "llama_model_loader: - kv  17:                       qwen2.context_length u32              = 131072\n",
            "llama_model_loader: - kv  18:                     qwen2.embedding_length u32              = 5120\n",
            "llama_model_loader: - kv  19:                  qwen2.feed_forward_length u32              = 13824\n",
            "llama_model_loader: - kv  20:                 qwen2.attention.head_count u32              = 40\n",
            "llama_model_loader: - kv  21:              qwen2.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  22:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv  23:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  24:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  25:                         tokenizer.ggml.pre str              = deepseek-r1-qwen\n",
            "llama_model_loader: - kv  26:                      tokenizer.ggml.tokens arr[str,151665]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  27:                  tokenizer.ggml.token_type arr[i32,151665]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  28:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
            "llama_model_loader: - kv  29:                tokenizer.ggml.bos_token_id u32              = 151646\n",
            "llama_model_loader: - kv  30:                tokenizer.ggml.eos_token_id u32              = 151643\n",
            "llama_model_loader: - kv  31:            tokenizer.ggml.padding_token_id u32              = 151643\n",
            "llama_model_loader: - kv  32:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  33:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  34:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\n",
            "llama_model_loader: - kv  35:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  36:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  37:                                general.url str              = https://huggingface.co/mradermacher/D...\n",
            "llama_model_loader: - kv  38:              mradermacher.quantize_version str              = 2\n",
            "llama_model_loader: - kv  39:                  mradermacher.quantized_by str              = mradermacher\n",
            "llama_model_loader: - kv  40:                  mradermacher.quantized_at str              = 2025-01-25T02:56:59+01:00\n",
            "llama_model_loader: - kv  41:                  mradermacher.quantized_on str              = marco\n",
            "llama_model_loader: - kv  42:                         general.source.url str              = https://huggingface.co/nicoboss/DeepS...\n",
            "llama_model_loader: - kv  43:                  mradermacher.convert_type str              = hf\n",
            "llama_model_loader: - type  f32:  241 tensors\n",
            "llama_model_loader: - type q4_K:  289 tensors\n",
            "llama_model_loader: - type q6_K:   49 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = Q4_K - Medium\n",
            "print_info: file size   = 8.36 GiB (4.86 BPW) \n",
            "init_tokenizer: initializing tokenizer for type 2\n",
            "load: control token: 151660 '<|fim_middle|>' is not marked as EOG\n",
            "load: control token: 151659 '<|fim_prefix|>' is not marked as EOG\n",
            "load: control token: 151653 '<|vision_end|>' is not marked as EOG\n",
            "load: control token: 151645 '<｜Assistant｜>' is not marked as EOG\n",
            "load: control token: 151644 '<｜User｜>' is not marked as EOG\n",
            "load: control token: 151655 '<|image_pad|>' is not marked as EOG\n",
            "load: control token: 151651 '<|quad_end|>' is not marked as EOG\n",
            "load: control token: 151646 '<｜begin▁of▁sentence｜>' is not marked as EOG\n",
            "load: control token: 151643 '<｜end▁of▁sentence｜>' is not marked as EOG\n",
            "load: control token: 151652 '<|vision_start|>' is not marked as EOG\n",
            "load: control token: 151647 '<|EOT|>' is not marked as EOG\n",
            "load: control token: 151654 '<|vision_pad|>' is not marked as EOG\n",
            "load: control token: 151656 '<|video_pad|>' is not marked as EOG\n",
            "load: control token: 151661 '<|fim_suffix|>' is not marked as EOG\n",
            "load: control token: 151650 '<|quad_start|>' is not marked as EOG\n",
            "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
            "load: special tokens cache size = 22\n",
            "load: token to piece cache size = 0.9310 MB\n",
            "print_info: arch             = qwen2\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 131072\n",
            "print_info: n_embd           = 5120\n",
            "print_info: n_layer          = 48\n",
            "print_info: n_head           = 40\n",
            "print_info: n_head_kv        = 8\n",
            "print_info: n_rot            = 128\n",
            "print_info: n_swa            = 0\n",
            "print_info: n_embd_head_k    = 128\n",
            "print_info: n_embd_head_v    = 128\n",
            "print_info: n_gqa            = 5\n",
            "print_info: n_embd_k_gqa     = 1024\n",
            "print_info: n_embd_v_gqa     = 1024\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-05\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: n_ff             = 13824\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 2\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 1000000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 131072\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: ssm_d_conv       = 0\n",
            "print_info: ssm_d_inner      = 0\n",
            "print_info: ssm_d_state      = 0\n",
            "print_info: ssm_dt_rank      = 0\n",
            "print_info: ssm_dt_b_c_rms   = 0\n",
            "print_info: model type       = 14B\n",
            "print_info: model params     = 14.77 B\n",
            "print_info: general.name     = DeepSeek R1 Distill Qwen 14B Uncensored\n",
            "print_info: vocab type       = BPE\n",
            "print_info: n_vocab          = 151665\n",
            "print_info: n_merges         = 151387\n",
            "print_info: BOS token        = 151646 '<｜begin▁of▁sentence｜>'\n",
            "print_info: EOS token        = 151643 '<｜end▁of▁sentence｜>'\n",
            "print_info: EOT token        = 151643 '<｜end▁of▁sentence｜>'\n",
            "print_info: PAD token        = 151643 '<｜end▁of▁sentence｜>'\n",
            "print_info: LF token         = 148848 'ÄĬ'\n",
            "print_info: FIM PRE token    = 151659 '<|fim_prefix|>'\n",
            "print_info: FIM SUF token    = 151661 '<|fim_suffix|>'\n",
            "print_info: FIM MID token    = 151660 '<|fim_middle|>'\n",
            "print_info: FIM PAD token    = 151662 '<|fim_pad|>'\n",
            "print_info: FIM REP token    = 151663 '<|repo_name|>'\n",
            "print_info: FIM SEP token    = 151664 '<|file_sep|>'\n",
            "print_info: EOG token        = 151643 '<｜end▁of▁sentence｜>'\n",
            "print_info: EOG token        = 151662 '<|fim_pad|>'\n",
            "print_info: EOG token        = 151663 '<|repo_name|>'\n",
            "print_info: EOG token        = 151664 '<|file_sep|>'\n",
            "print_info: max token length = 256\n",
            "load_tensors: layer   0 assigned to device CUDA0\n",
            "load_tensors: layer   1 assigned to device CUDA0\n",
            "load_tensors: layer   2 assigned to device CUDA0\n",
            "load_tensors: layer   3 assigned to device CUDA0\n",
            "load_tensors: layer   4 assigned to device CUDA0\n",
            "load_tensors: layer   5 assigned to device CUDA0\n",
            "load_tensors: layer   6 assigned to device CUDA0\n",
            "load_tensors: layer   7 assigned to device CUDA0\n",
            "load_tensors: layer   8 assigned to device CUDA0\n",
            "load_tensors: layer   9 assigned to device CUDA0\n",
            "load_tensors: layer  10 assigned to device CUDA0\n",
            "load_tensors: layer  11 assigned to device CUDA0\n",
            "load_tensors: layer  12 assigned to device CUDA0\n",
            "load_tensors: layer  13 assigned to device CUDA0\n",
            "load_tensors: layer  14 assigned to device CUDA0\n",
            "load_tensors: layer  15 assigned to device CUDA0\n",
            "load_tensors: layer  16 assigned to device CUDA0\n",
            "load_tensors: layer  17 assigned to device CUDA0\n",
            "load_tensors: layer  18 assigned to device CUDA0\n",
            "load_tensors: layer  19 assigned to device CUDA0\n",
            "load_tensors: layer  20 assigned to device CUDA0\n",
            "load_tensors: layer  21 assigned to device CUDA0\n",
            "load_tensors: layer  22 assigned to device CUDA0\n",
            "load_tensors: layer  23 assigned to device CUDA0\n",
            "load_tensors: layer  24 assigned to device CUDA0\n",
            "load_tensors: layer  25 assigned to device CUDA0\n",
            "load_tensors: layer  26 assigned to device CUDA0\n",
            "load_tensors: layer  27 assigned to device CUDA0\n",
            "load_tensors: layer  28 assigned to device CUDA0\n",
            "load_tensors: layer  29 assigned to device CUDA0\n",
            "load_tensors: layer  30 assigned to device CUDA0\n",
            "load_tensors: layer  31 assigned to device CUDA0\n",
            "load_tensors: layer  32 assigned to device CUDA0\n",
            "load_tensors: layer  33 assigned to device CUDA0\n",
            "load_tensors: layer  34 assigned to device CUDA0\n",
            "load_tensors: layer  35 assigned to device CUDA0\n",
            "load_tensors: layer  36 assigned to device CUDA0\n",
            "load_tensors: layer  37 assigned to device CUDA0\n",
            "load_tensors: layer  38 assigned to device CUDA0\n",
            "load_tensors: layer  39 assigned to device CUDA0\n",
            "load_tensors: layer  40 assigned to device CUDA0\n",
            "load_tensors: layer  41 assigned to device CUDA0\n",
            "load_tensors: layer  42 assigned to device CUDA0\n",
            "load_tensors: layer  43 assigned to device CUDA0\n",
            "load_tensors: layer  44 assigned to device CUDA0\n",
            "load_tensors: layer  45 assigned to device CUDA0\n",
            "load_tensors: layer  46 assigned to device CUDA0\n",
            "load_tensors: layer  47 assigned to device CUDA0\n",
            "load_tensors: layer  48 assigned to device CUDA0\n",
            "load_tensors: tensor 'token_embd.weight' (q4_K) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
            "load_tensors: offloading 48 repeating layers to GPU\n",
            "load_tensors: offloading output layer to GPU\n",
            "load_tensors: offloaded 49/49 layers to GPU\n",
            "load_tensors:        CUDA0 model buffer size =  8146.78 MiB\n",
            "load_tensors:   CPU_Mapped model buffer size =   416.56 MiB\n",
            "llama_init_from_model: n_seq_max     = 1\n",
            "llama_init_from_model: n_ctx         = 131072\n",
            "llama_init_from_model: n_ctx_per_seq = 131072\n",
            "llama_init_from_model: n_batch       = 512\n",
            "llama_init_from_model: n_ubatch      = 512\n",
            "llama_init_from_model: flash_attn    = 1\n",
            "llama_init_from_model: freq_base     = 1000000.0\n",
            "llama_init_from_model: freq_scale    = 1\n",
            "llama_kv_cache_init: kv_size = 131072, offload = 1, type_k = 'q4_0', type_v = 'q4_0', n_layer = 48, can_shift = 1\n",
            "llama_kv_cache_init: layer 0: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 1: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 2: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 3: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 4: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 5: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 6: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 7: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 8: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 9: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 10: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 11: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 12: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 13: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 14: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 15: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 16: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 17: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 18: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 19: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 20: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 21: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 22: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 23: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 24: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 25: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 26: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 27: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 28: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 29: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 30: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 31: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 32: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 33: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 34: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 35: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 36: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 37: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 38: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 39: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 40: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 41: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 42: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 43: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 44: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 45: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 46: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 47: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "ggml_backend_cuda_buffer_type_alloc_buffer: allocating 6912.00 MiB on device 0: cudaMalloc failed: out of memory\n",
            "llama_kv_cache_init: failed to allocate buffer for kv cache\n",
            "llama_init_from_model: llama_kv_cache_init() failed for self-attention cache\n",
            "\u001b[2;36m21:20:05-517493\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;31mERROR   \u001b[0m Failed to load the model.                                                  \n",
            "Traceback (most recent call last):\n",
            "  File \"/content/text-generation-webui/modules/llamacpp_model.py\", line 112, in from_pretrained\n",
            "    result.model = Llama(**params)\n",
            "                   ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/llama_cpp_cuda/llama.py\", line 393, in __init__\n",
            "    internals.LlamaContext(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/llama_cpp_cuda/_internals.py\", line 255, in __init__\n",
            "    raise ValueError(\"Failed to create llama_context\")\n",
            "ValueError: Failed to create llama_context\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/text-generation-webui/modules/ui_model_menu.py\", line 214, in load_model_wrapper\n",
            "    shared.model, shared.tokenizer = load_model(selected_model, loader)\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/text-generation-webui/modules/models.py\", line 90, in load_model\n",
            "    output = load_func_map[loader](model_name)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/text-generation-webui/modules/models.py\", line 280, in llamacpp_loader\n",
            "    model, tokenizer = LlamaCppModel.from_pretrained(model_file)\n",
            "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/text-generation-webui/modules/llamacpp_model.py\", line 122, in from_pretrained\n",
            "    raise type(e)(error_message) from e\n",
            "ValueError: Failed loading the model. **This usually happens due to lack of memory**. Try these steps:\n",
            "1. Reduce the context length `n_ctx` (currently 131072). Try a lower value like 4096.\n",
            "2. Lower the `n-gpu-layers` value (currently 49).\n",
            "\n",
            "Exception ignored in: <function LlamaCppModel.__del__ at 0x7871d07f8a40>\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/text-generation-webui/modules/llamacpp_model.py\", line 62, in __del__\n",
            "    del self.model\n",
            "        ^^^^^^^^^^\n",
            "AttributeError: 'LlamaCppModel' object has no attribute 'model'\n",
            "\u001b[2;36m21:20:28-548418\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading \u001b[32m\"mradermacher_DeepSeek-R1-Distill-Qwen-14B-Uncensored-GGUF\"\u001b[0m        \n",
            "\u001b[2;36m21:20:29-096000\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m llama.cpp weights detected:                                                \n",
            "\u001b[2;36m                \u001b[0m         \u001b[32m\"models/mradermacher_DeepSeek-R1-Distill-Qwen-14B-Uncensored-GGUF/DeepSeek-\u001b[0m\n",
            "\u001b[2;36m                \u001b[0m         \u001b[32mR1-Distill-Qwen-14B-Uncensored.Q4_K_M.gguf\"\u001b[0m                                \n",
            "llama_model_load_from_file_impl: using device CUDA0 (Tesla T4) - 14992 MiB free\n",
            "llama_model_loader: loaded meta data with 44 key-value pairs and 579 tensors from models/mradermacher_DeepSeek-R1-Distill-Qwen-14B-Uncensored-GGUF/DeepSeek-R1-Distill-Qwen-14B-Uncensored.Q4_K_M.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 14B Uncensored\n",
            "llama_model_loader: - kv   3:                           general.finetune str              = Uncensored\n",
            "llama_model_loader: - kv   4:                           general.basename str              = DeepSeek-R1-Distill-Qwen\n",
            "llama_model_loader: - kv   5:                         general.size_label str              = 14B\n",
            "llama_model_loader: - kv   6:                            general.license str              = mit\n",
            "llama_model_loader: - kv   7:                   general.base_model.count u32              = 1\n",
            "llama_model_loader: - kv   8:                  general.base_model.0.name str              = DeepSeek R1 Distill Qwen 14B\n",
            "llama_model_loader: - kv   9:          general.base_model.0.organization str              = Deepseek Ai\n",
            "llama_model_loader: - kv  10:              general.base_model.0.repo_url str              = https://huggingface.co/deepseek-ai/De...\n",
            "llama_model_loader: - kv  11:                      general.dataset.count u32              = 1\n",
            "llama_model_loader: - kv  12:                     general.dataset.0.name str              = Uncensor\n",
            "llama_model_loader: - kv  13:             general.dataset.0.organization str              = Guilherme34\n",
            "llama_model_loader: - kv  14:                 general.dataset.0.repo_url str              = https://huggingface.co/Guilherme34/un...\n",
            "llama_model_loader: - kv  15:                               general.tags arr[str,1]       = [\"generated_from_trainer\"]\n",
            "llama_model_loader: - kv  16:                          qwen2.block_count u32              = 48\n",
            "llama_model_loader: - kv  17:                       qwen2.context_length u32              = 131072\n",
            "llama_model_loader: - kv  18:                     qwen2.embedding_length u32              = 5120\n",
            "llama_model_loader: - kv  19:                  qwen2.feed_forward_length u32              = 13824\n",
            "llama_model_loader: - kv  20:                 qwen2.attention.head_count u32              = 40\n",
            "llama_model_loader: - kv  21:              qwen2.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  22:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv  23:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  24:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  25:                         tokenizer.ggml.pre str              = deepseek-r1-qwen\n",
            "llama_model_loader: - kv  26:                      tokenizer.ggml.tokens arr[str,151665]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  27:                  tokenizer.ggml.token_type arr[i32,151665]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  28:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
            "llama_model_loader: - kv  29:                tokenizer.ggml.bos_token_id u32              = 151646\n",
            "llama_model_loader: - kv  30:                tokenizer.ggml.eos_token_id u32              = 151643\n",
            "llama_model_loader: - kv  31:            tokenizer.ggml.padding_token_id u32              = 151643\n",
            "llama_model_loader: - kv  32:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  33:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  34:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\n",
            "llama_model_loader: - kv  35:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  36:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  37:                                general.url str              = https://huggingface.co/mradermacher/D...\n",
            "llama_model_loader: - kv  38:              mradermacher.quantize_version str              = 2\n",
            "llama_model_loader: - kv  39:                  mradermacher.quantized_by str              = mradermacher\n",
            "llama_model_loader: - kv  40:                  mradermacher.quantized_at str              = 2025-01-25T02:56:59+01:00\n",
            "llama_model_loader: - kv  41:                  mradermacher.quantized_on str              = marco\n",
            "llama_model_loader: - kv  42:                         general.source.url str              = https://huggingface.co/nicoboss/DeepS...\n",
            "llama_model_loader: - kv  43:                  mradermacher.convert_type str              = hf\n",
            "llama_model_loader: - type  f32:  241 tensors\n",
            "llama_model_loader: - type q4_K:  289 tensors\n",
            "llama_model_loader: - type q6_K:   49 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = Q4_K - Medium\n",
            "print_info: file size   = 8.36 GiB (4.86 BPW) \n",
            "init_tokenizer: initializing tokenizer for type 2\n",
            "load: control token: 151660 '<|fim_middle|>' is not marked as EOG\n",
            "load: control token: 151659 '<|fim_prefix|>' is not marked as EOG\n",
            "load: control token: 151653 '<|vision_end|>' is not marked as EOG\n",
            "load: control token: 151645 '<｜Assistant｜>' is not marked as EOG\n",
            "load: control token: 151644 '<｜User｜>' is not marked as EOG\n",
            "load: control token: 151655 '<|image_pad|>' is not marked as EOG\n",
            "load: control token: 151651 '<|quad_end|>' is not marked as EOG\n",
            "load: control token: 151646 '<｜begin▁of▁sentence｜>' is not marked as EOG\n",
            "load: control token: 151643 '<｜end▁of▁sentence｜>' is not marked as EOG\n",
            "load: control token: 151652 '<|vision_start|>' is not marked as EOG\n",
            "load: control token: 151647 '<|EOT|>' is not marked as EOG\n",
            "load: control token: 151654 '<|vision_pad|>' is not marked as EOG\n",
            "load: control token: 151656 '<|video_pad|>' is not marked as EOG\n",
            "load: control token: 151661 '<|fim_suffix|>' is not marked as EOG\n",
            "load: control token: 151650 '<|quad_start|>' is not marked as EOG\n",
            "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
            "load: special tokens cache size = 22\n",
            "load: token to piece cache size = 0.9310 MB\n",
            "print_info: arch             = qwen2\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 131072\n",
            "print_info: n_embd           = 5120\n",
            "print_info: n_layer          = 48\n",
            "print_info: n_head           = 40\n",
            "print_info: n_head_kv        = 8\n",
            "print_info: n_rot            = 128\n",
            "print_info: n_swa            = 0\n",
            "print_info: n_embd_head_k    = 128\n",
            "print_info: n_embd_head_v    = 128\n",
            "print_info: n_gqa            = 5\n",
            "print_info: n_embd_k_gqa     = 1024\n",
            "print_info: n_embd_v_gqa     = 1024\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-05\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: n_ff             = 13824\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 2\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 1000000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 131072\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: ssm_d_conv       = 0\n",
            "print_info: ssm_d_inner      = 0\n",
            "print_info: ssm_d_state      = 0\n",
            "print_info: ssm_dt_rank      = 0\n",
            "print_info: ssm_dt_b_c_rms   = 0\n",
            "print_info: model type       = 14B\n",
            "print_info: model params     = 14.77 B\n",
            "print_info: general.name     = DeepSeek R1 Distill Qwen 14B Uncensored\n",
            "print_info: vocab type       = BPE\n",
            "print_info: n_vocab          = 151665\n",
            "print_info: n_merges         = 151387\n",
            "print_info: BOS token        = 151646 '<｜begin▁of▁sentence｜>'\n",
            "print_info: EOS token        = 151643 '<｜end▁of▁sentence｜>'\n",
            "print_info: EOT token        = 151643 '<｜end▁of▁sentence｜>'\n",
            "print_info: PAD token        = 151643 '<｜end▁of▁sentence｜>'\n",
            "print_info: LF token         = 148848 'ÄĬ'\n",
            "print_info: FIM PRE token    = 151659 '<|fim_prefix|>'\n",
            "print_info: FIM SUF token    = 151661 '<|fim_suffix|>'\n",
            "print_info: FIM MID token    = 151660 '<|fim_middle|>'\n",
            "print_info: FIM PAD token    = 151662 '<|fim_pad|>'\n",
            "print_info: FIM REP token    = 151663 '<|repo_name|>'\n",
            "print_info: FIM SEP token    = 151664 '<|file_sep|>'\n",
            "print_info: EOG token        = 151643 '<｜end▁of▁sentence｜>'\n",
            "print_info: EOG token        = 151662 '<|fim_pad|>'\n",
            "print_info: EOG token        = 151663 '<|repo_name|>'\n",
            "print_info: EOG token        = 151664 '<|file_sep|>'\n",
            "print_info: max token length = 256\n",
            "load_tensors: layer   0 assigned to device CUDA0\n",
            "load_tensors: layer   1 assigned to device CUDA0\n",
            "load_tensors: layer   2 assigned to device CUDA0\n",
            "load_tensors: layer   3 assigned to device CUDA0\n",
            "load_tensors: layer   4 assigned to device CUDA0\n",
            "load_tensors: layer   5 assigned to device CUDA0\n",
            "load_tensors: layer   6 assigned to device CUDA0\n",
            "load_tensors: layer   7 assigned to device CUDA0\n",
            "load_tensors: layer   8 assigned to device CUDA0\n",
            "load_tensors: layer   9 assigned to device CUDA0\n",
            "load_tensors: layer  10 assigned to device CUDA0\n",
            "load_tensors: layer  11 assigned to device CUDA0\n",
            "load_tensors: layer  12 assigned to device CUDA0\n",
            "load_tensors: layer  13 assigned to device CUDA0\n",
            "load_tensors: layer  14 assigned to device CUDA0\n",
            "load_tensors: layer  15 assigned to device CUDA0\n",
            "load_tensors: layer  16 assigned to device CUDA0\n",
            "load_tensors: layer  17 assigned to device CUDA0\n",
            "load_tensors: layer  18 assigned to device CUDA0\n",
            "load_tensors: layer  19 assigned to device CUDA0\n",
            "load_tensors: layer  20 assigned to device CUDA0\n",
            "load_tensors: layer  21 assigned to device CUDA0\n",
            "load_tensors: layer  22 assigned to device CUDA0\n",
            "load_tensors: layer  23 assigned to device CUDA0\n",
            "load_tensors: layer  24 assigned to device CUDA0\n",
            "load_tensors: layer  25 assigned to device CUDA0\n",
            "load_tensors: layer  26 assigned to device CUDA0\n",
            "load_tensors: layer  27 assigned to device CUDA0\n",
            "load_tensors: layer  28 assigned to device CUDA0\n",
            "load_tensors: layer  29 assigned to device CUDA0\n",
            "load_tensors: layer  30 assigned to device CUDA0\n",
            "load_tensors: layer  31 assigned to device CUDA0\n",
            "load_tensors: layer  32 assigned to device CUDA0\n",
            "load_tensors: layer  33 assigned to device CUDA0\n",
            "load_tensors: layer  34 assigned to device CUDA0\n",
            "load_tensors: layer  35 assigned to device CUDA0\n",
            "load_tensors: layer  36 assigned to device CUDA0\n",
            "load_tensors: layer  37 assigned to device CUDA0\n",
            "load_tensors: layer  38 assigned to device CUDA0\n",
            "load_tensors: layer  39 assigned to device CUDA0\n",
            "load_tensors: layer  40 assigned to device CUDA0\n",
            "load_tensors: layer  41 assigned to device CUDA0\n",
            "load_tensors: layer  42 assigned to device CUDA0\n",
            "load_tensors: layer  43 assigned to device CUDA0\n",
            "load_tensors: layer  44 assigned to device CUDA0\n",
            "load_tensors: layer  45 assigned to device CUDA0\n",
            "load_tensors: layer  46 assigned to device CUDA0\n",
            "load_tensors: layer  47 assigned to device CUDA0\n",
            "load_tensors: layer  48 assigned to device CUDA0\n",
            "load_tensors: tensor 'token_embd.weight' (q4_K) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
            "load_tensors: offloading 48 repeating layers to GPU\n",
            "load_tensors: offloading output layer to GPU\n",
            "load_tensors: offloaded 49/49 layers to GPU\n",
            "load_tensors:        CUDA0 model buffer size =  8146.78 MiB\n",
            "load_tensors:   CPU_Mapped model buffer size =   416.56 MiB\n",
            "llama_init_from_model: n_seq_max     = 1\n",
            "llama_init_from_model: n_ctx         = 32768\n",
            "llama_init_from_model: n_ctx_per_seq = 32768\n",
            "llama_init_from_model: n_batch       = 512\n",
            "llama_init_from_model: n_ubatch      = 512\n",
            "llama_init_from_model: flash_attn    = 1\n",
            "llama_init_from_model: freq_base     = 1000000.0\n",
            "llama_init_from_model: freq_scale    = 1\n",
            "llama_init_from_model: n_ctx_per_seq (32768) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_kv_cache_init: kv_size = 32768, offload = 1, type_k = 'q4_0', type_v = 'q4_0', n_layer = 48, can_shift = 1\n",
            "llama_kv_cache_init: layer 0: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 1: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 2: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 3: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 4: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 5: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 6: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 7: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 8: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 9: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 10: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 11: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 12: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 13: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 14: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 15: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 16: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 17: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 18: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 19: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 20: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 21: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 22: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 23: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 24: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 25: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 26: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 27: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 28: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 29: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 30: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 31: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 32: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 33: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 34: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 35: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 36: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 37: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 38: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 39: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 40: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 41: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 42: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 43: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 44: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 45: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 46: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 47: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =  1728.00 MiB\n",
            "llama_init_from_model: KV self size  = 1728.00 MiB, K (q4_0):  864.00 MiB, V (q4_0):  864.00 MiB\n",
            "llama_init_from_model:  CUDA_Host  output buffer size =     0.58 MiB\n",
            "llama_init_from_model:      CUDA0 compute buffer size =   306.22 MiB\n",
            "llama_init_from_model:  CUDA_Host compute buffer size =    74.01 MiB\n",
            "llama_init_from_model: graph nodes  = 1495\n",
            "llama_init_from_model: graph splits = 2\n",
            "CUDA : ARCHS = 500,520,530,600,610,620,700,720,750,800,860,870,890,900 | FORCE_MMQ = 1 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
            "Model metadata: {'mradermacher.quantized_on': 'marco', 'mradermacher.quantized_at': '2025-01-25T02:56:59+01:00', 'mradermacher.quantized_by': 'mradermacher', 'mradermacher.quantize_version': '2', 'general.url': 'https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-14B-Uncensored-GGUF', 'general.quantization_version': '2', 'tokenizer.chat_template': \"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='') %}{%- for message in messages %}{%- if message['role'] == 'system' %}{% set ns.system_prompt = message['content'] %}{%- endif %}{%- endfor %}{{bos_token}}{{ns.system_prompt}}{%- for message in messages %}{%- if message['role'] == 'user' %}{%- set ns.is_tool = false -%}{{'<｜User｜>' + message['content']}}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is none %}{%- set ns.is_tool = false -%}{%- for tool in message['tool_calls']%}{%- if not ns.is_first %}{{'<｜Assistant｜><｜tool▁calls▁begin｜><｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\\\n' + '```json' + '\\\\n' + tool['function']['arguments'] + '\\\\n' + '```' + '<｜tool▁call▁end｜>'}}{%- set ns.is_first = true -%}{%- else %}{{'\\\\n' + '<｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\\\n' + '```json' + '\\\\n' + tool['function']['arguments'] + '\\\\n' + '```' + '<｜tool▁call▁end｜>'}}{{'<｜tool▁calls▁end｜><｜end▁of▁sentence｜>'}}{%- endif %}{%- endfor %}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is not none %}{%- if ns.is_tool %}{{'<｜tool▁outputs▁end｜>' + message['content'] + '<｜end▁of▁sentence｜>'}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message['content'] %}{% if '</think>' in content %}{% set content = content.split('</think>')[-1] %}{% endif %}{{'<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>'}}{%- endif %}{%- endif %}{%- if message['role'] == 'tool' %}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{'<｜tool▁outputs▁begin｜><｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- set ns.is_output_first = false %}{%- else %}{{'\\\\n<｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{'<｜tool▁outputs▁end｜>'}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{'<｜Assistant｜>'}}{% endif %}\", 'tokenizer.ggml.add_eos_token': 'false', 'general.dataset.0.name': 'Uncensor', 'general.dataset.count': '1', 'qwen2.attention.layer_norm_rms_epsilon': '0.000010', 'general.base_model.0.repo_url': 'https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B', 'general.type': 'model', 'general.file_type': '15', 'general.dataset.0.organization': 'Guilherme34', 'general.finetune': 'Uncensored', 'general.base_model.0.name': 'DeepSeek R1 Distill Qwen 14B', 'tokenizer.ggml.pre': 'deepseek-r1-qwen', 'mradermacher.convert_type': 'hf', 'general.source.url': 'https://huggingface.co/nicoboss/DeepSeek-R1-Distill-Qwen-14B-Uncensored', 'general.base_model.count': '1', 'general.base_model.0.organization': 'Deepseek Ai', 'general.size_label': '14B', 'tokenizer.ggml.add_bos_token': 'true', 'general.basename': 'DeepSeek-R1-Distill-Qwen', 'qwen2.embedding_length': '5120', 'tokenizer.ggml.padding_token_id': '151643', 'qwen2.block_count': '48', 'general.architecture': 'qwen2', 'qwen2.context_length': '131072', 'qwen2.feed_forward_length': '13824', 'tokenizer.ggml.model': 'gpt2', 'qwen2.attention.head_count': '40', 'general.license': 'mit', 'qwen2.attention.head_count_kv': '8', 'qwen2.rope.freq_base': '1000000.000000', 'general.dataset.0.repo_url': 'https://huggingface.co/Guilherme34/uncensor', 'tokenizer.ggml.eos_token_id': '151643', 'general.name': 'DeepSeek R1 Distill Qwen 14B Uncensored', 'tokenizer.ggml.bos_token_id': '151646'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Using gguf chat template: {% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='') %}{%- for message in messages %}{%- if message['role'] == 'system' %}{% set ns.system_prompt = message['content'] %}{%- endif %}{%- endfor %}{{bos_token}}{{ns.system_prompt}}{%- for message in messages %}{%- if message['role'] == 'user' %}{%- set ns.is_tool = false -%}{{'<｜User｜>' + message['content']}}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is none %}{%- set ns.is_tool = false -%}{%- for tool in message['tool_calls']%}{%- if not ns.is_first %}{{'<｜Assistant｜><｜tool▁calls▁begin｜><｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<｜tool▁call▁end｜>'}}{%- set ns.is_first = true -%}{%- else %}{{'\\n' + '<｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<｜tool▁call▁end｜>'}}{{'<｜tool▁calls▁end｜><｜end▁of▁sentence｜>'}}{%- endif %}{%- endfor %}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is not none %}{%- if ns.is_tool %}{{'<｜tool▁outputs▁end｜>' + message['content'] + '<｜end▁of▁sentence｜>'}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message['content'] %}{% if '</think>' in content %}{% set content = content.split('</think>')[-1] %}{% endif %}{{'<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>'}}{%- endif %}{%- endif %}{%- if message['role'] == 'tool' %}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{'<｜tool▁outputs▁begin｜><｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- set ns.is_output_first = false %}{%- else %}{{'\\n<｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{'<｜tool▁outputs▁end｜>'}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{'<｜Assistant｜>'}}{% endif %}\n",
            "Using chat eos_token: <｜end▁of▁sentence｜>\n",
            "Using chat bos_token: <｜begin▁of▁sentence｜>\n",
            "\u001b[2;36m21:20:32-171307\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loaded \u001b[32m\"mradermacher_DeepSeek-R1-Distill-Qwen-14B-Uncensored-GGUF\"\u001b[0m in \u001b[1;36m3.62\u001b[0m \n",
            "\u001b[2;36m                \u001b[0m         seconds.                                                                   \n",
            "\u001b[2;36m21:20:32-172409\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m LOADER: \u001b[32m\"llama.cpp\"\u001b[0m                                                        \n",
            "\u001b[2;36m21:20:32-173203\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m TRUNCATION LENGTH: \u001b[1;36m32768\u001b[0m                                                   \n",
            "\u001b[2;36m21:20:32-174010\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m INSTRUCTION TEMPLATE: \u001b[32m\"Custom \u001b[0m\u001b[32m(\u001b[0m\u001b[32mobtained from model metadata\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\"\u001b[0m              \n",
            "/usr/local/lib/python3.11/dist-packages/llama_cpp_cuda/llama.py:1240: RuntimeWarning: Detected duplicate leading \"<｜begin▁of▁sentence｜>\" in prompt, this will likely reduce response quality, consider removing it...\n",
            "  warnings.warn(\n",
            "Output generated in 6.70 seconds (14.32 tokens/s, 96 tokens, context 108, seed 1099678092)\n"
          ]
        }
      ]
    }
  ]
}